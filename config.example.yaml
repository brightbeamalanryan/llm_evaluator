# Example configuration for eval-fw
# Copy this to config.yaml and modify as needed

# Target model to evaluate (the model under test)
target:
  type: ollama  # Options: ollama, openai, anthropic
  model: phi
  # api_key: ${OPENAI_API_KEY}  # For cloud providers
  # base_url: http://localhost:11434/api/chat  # For custom endpoints
  temperature: 0.3
  top_p: 0.9
  timeout: 600

# Guard model for classification (evaluates target responses)
guard:
  type: ollama
  model: llama3.1:8b
  temperature: 0.1  # Lower temperature for more consistent classification

# Mutator model for RAG prompt evolution
mutator:
  enabled: true
  type: ollama
  model: prompt-mutator
  # base_url: http://localhost:11434/api/chat
  temperature: 0.7
  top_p: 0.9
  timeout: 600
  max_iterations: 100
  plateau_window: 10
  plateau_tolerance: 0.01
  stop_score_threshold: 1.0

# Path to test cases (JSON or YAML)
tests_path: ./use_cases/tests.json

# Optional: Track which tests have been run
state_file: ./use_cases/db.json

# Number of concurrent tests (for async mode)
concurrency: 5

# Report configuration
report:
  output_dir: ./reports
  formats:
    - json
    - html
  # html_template: ./custom_template.html  # Optional custom template

# Logging directory
log_dir: ./logs

# RAG test configuration
#  retrieve_endpoint: /retrieve
#  ingest_endpoint: /ingest
rag:
  tests_path: /home/alan/devel/ai_sec/apps/llm_evaluator/use_cases/rag_tests.json
  service_url: http://localhost:8000
  query_endpoint: /query
  endpoint_mode: query
